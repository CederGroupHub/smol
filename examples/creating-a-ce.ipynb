{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a basic Cluster Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from monty.serialization import loadfn, dumpfn\n",
    "from pymatgen.core.structure import Structure\n",
    "from smol.cofe import ClusterSubspace, StructureWrangler, ClusterExpansion, RegressionData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prim structure\n",
    "lno_prim = loadfn('data/lno_prim.json')\n",
    "    \n",
    "# load the fitting data\n",
    "lno_entries = loadfn(\"data/lno_entries.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) The prim structure\n",
    "The prim structure defines the **configurational space** for the Cluster Expansion. \n",
    "The **configurational space** is defined by the site **compositional spaces** and the crystal symetries of the prim structure.\n",
    "The occupancy of the sites determine site **compositional spaces**. Sites are **active** if they have compositional degrees of freedom.\n",
    "\n",
    "\n",
    "Active sites have fractional compositions. Vacancies are allowed in sites where the composition does not sum to one.\n",
    "\n",
    "0. Is active. The allowed species are: Li+ and vacancies.\n",
    "1. Is active. The allowed species are: Ni3+ and Ni4+.\n",
    "2. Is not active. Only O2- is allowed.\n",
    "3. Is not active. Only O2- is allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lno_prim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) The cluster subspace\n",
    "The `ClusterSubspace` represents all the orbits (groups of equivalent clusters) that will be considered when fitting the cluster expansion. Its main purpose is to compute the **correlations functions** for each included orbit given a structure in the compositional space defined by the prim.\n",
    "\n",
    "In order to do be able to compute the correlation functions, the given structure must match the prim structure in a \"crystallographic\" sense but allowing for compositional degrees of freedom in the \"active\" sites.\n",
    "\n",
    "A cluster subspace most easily created by providing:\n",
    "1. The prim structure representing the configurational space.\n",
    "2. A set of diameter cutoffs for each size of orbit we want to consider.\n",
    "3. A type of site basis function to use.\n",
    "\n",
    "There are more options allowed by the code to fine grain and tune. See other notebooks for advanced use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subspace = ClusterSubspace.from_cutoffs(lno_prim,\n",
    "                                        cutoffs={2: 5, 3: 4.1}, # will include orbits of 2 and 3 sites.\n",
    "                                        basis='sinusoid', # sets the site basis type, default is indicator\n",
    "                                        supercell_size='O2-')\n",
    "\n",
    "# supercell_size specifies the method to determine the supercell size\n",
    "# when trying to match a structure.\n",
    "# (See pymatgen.structure_matcher.StructureMatcher for more info)\n",
    "\n",
    "print(subspace) # single site and empty orbits are always included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1) Computing a correlation vector.\n",
    "A correlation vector for a specific structure (represents the feature vector) used to train and predict target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = lno_entries[1].structure\n",
    "corr = subspace.corr_from_structure(structure)\n",
    "\n",
    "print(f'The correlation vector for a structure with'\n",
    "      f' composition {structure.composition} is: '\n",
    "      f'\\n{corr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) The structure wrangler\n",
    "The `StructureWrangler` is a class that will is used to create and organize the data that will be used to train (and possibly test) the cluster expansion. It makes sure that all the supplied structures appropriately match the prim structure, and obtains the necessary information to correctly normalize target properties (such as energy) necessary for training.\n",
    "\n",
    "Training data is added to a `StructureWrangler` using `ComputedStructureEntry` instances from `pymatgen`.\n",
    "\n",
    "Matching relaxed structures can be a tricky problem, especially for ionic systems with vacancies. See the notebook on structure matching for tips on how to tweak parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangler = StructureWrangler(subspace)\n",
    "\n",
    "# the energy is taken directly from the ComputedStructureEntry\n",
    "# any additional properties can also be added, see notebook on\n",
    "# training data preparation for an example.\n",
    "for entry in lno_entries:\n",
    "    wrangler.add_entry(entry, verbose=True)\n",
    "# The verbose flag will print structures that fail to match.\n",
    "\n",
    "print(f'\\nTotal structures that match {wrangler.num_structures}/{len(lno_entries)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Training\n",
    "\n",
    "Training a cluster expansion is one of the most critical steps. This is how you get **effective cluster interactions (ECI's)**. To do so you need an estimator class that implements some form of regression model. In this case we will use simple least squares regression using the `LinearRegression` estimator from `scikit-learn`.\n",
    "\n",
    "In `smol` the coefficients from the fit are not exactly the ECI's but the ECI times the multiplicity of their orbit.\n",
    "\n",
    "Currently we also have the old l1regs estimator from pyabinitio in `theorytoolkit.regression` as `WDRLasso`, but it will likely be deprecated and removed at some point..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Set fit_intercept to False because we already do this using\n",
    "# the empty cluster.\n",
    "estimator = LinearRegression(fit_intercept=False)\n",
    "estimator.fit(wrangler.feature_matrix,\n",
    "              wrangler.get_property_vector('energy'))\n",
    "coefs = estimator.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) Check the quality of the fit\n",
    "There are many ways to evaluate the quality of a fit. The simplest involve stadard training set prediction error metrics. But when evaluating a CE more seriously we need to consider further metrics and how the CE will be used.\n",
    "Here we will just look at in sample mean squared error and max error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, max_error\n",
    "\n",
    "train_predictions = np.dot(wrangler.feature_matrix, coefs)\n",
    "\n",
    "rmse = mean_squared_error(wrangler.get_property_vector('energy'),\n",
    "                          train_predictions, squared=False)\n",
    "maxer = max_error(wrangler.get_property_vector('energy'),\n",
    "                  train_predictions)\n",
    "\n",
    "print(f'RMSE {1E3 * rmse} meV/prim')\n",
    "print(f'MAX {1E3 * maxer} meV/prim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) The cluster expansion\n",
    "Now we can use the above work to create the `ClusterExpansion`. The cluster expansion can be used to predict the fitted property for new structures, either for testing quality or for simulations such as in Monte Carlo.\n",
    "Note that when using the `predict` function, the cluster expansion will have to match the given structure if it has not seen it before.\n",
    "We will also store the details of the regression model used to fit the cluster expansion by using a `RegressionData` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data = RegressionData.from_sklearn(\n",
    "    estimator, wrangler.feature_matrix,\n",
    "    wrangler.get_property_vector('energy'))\n",
    "\n",
    "\n",
    "expansion = ClusterExpansion(subspace,\n",
    "                             coefficients=coefs,\n",
    "                             regression_data=reg_data)\n",
    "\n",
    "structure = random.choice(wrangler.structures)\n",
    "prediction = expansion.predict(structure, normalize=True)\n",
    "\n",
    "print(f'The predicted energy for a structure with composition '\n",
    "      f'{structure.composition} is {prediction} eV/prim.\\n')\n",
    "print(f'The fitted coefficients are:\\n{expansion.coefs}\\n')\n",
    "print(f'The effective cluster interactions are:\\n{expansion.eci}\\n')\n",
    "print(expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Saving your work\n",
    "All core classes in `smol` are `MSONables` and so can be saved using their `as_dict` methods or better yet with `monty.serialization.dumpfn`.\n",
    "\n",
    "Currently there is also a convenience function in `smol` that will nicely save all of your work for you in a standardized way. Work saved with the `save_work` function is saved as a dictionary with standardized names for the classes. Since a work flow should only contain 1 of each core classes the function will complain if you give it two of the same class (i.e. two wranglers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smol.io import save_work\n",
    "\n",
    "file_path = 'data/basic_ce.mson'\n",
    "# we can save the subspace as well, but since both the wrangler\n",
    "# and the expansion have it, there is no need to do so.\n",
    "save_work(file_path, wrangler, expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1) Loading previously saved work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smol.io import load_work\n",
    "\n",
    "work = load_work(file_path)\n",
    "for name, obj in work.items():\n",
    "    print(f'{name}: {type(obj)}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matx_dev",
   "language": "python",
   "name": "matx_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
